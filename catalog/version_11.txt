
    DATE : 20/02/2023 01:17:04
    version_catalog : 11
    version_data : 11
    version_model : 11
    train_batch_size : 128
    test_batch_size : 128
    
    num_gpu : 4
    limit : Nonetrainpath
    epoch : 100
    mode : train
    
    checkpoint_path : /data/stg60/karl_transferlearningModel/version_11/model.ckpt
    
    data_root = /data/stg60/transfer_learningdata/Karl/version_11

    trainpath = /data/stg60/transfer_learningdata/Karl/version_11 / 'train.npz'
    valpath = /data/stg60/transfer_learningdata/Karl/version_11 / 'val.npz'
    testpath = /data/stg60/transfer_learningdata/Karl/version_11 / 'par.npz'
    predpath = /data/stg60/transfer_learningdata/Karl/version_11 / 'pred.npz'
    
    

        ########################################################
        ###############       preparedata_newmilton.py       #############
        ########################################################
        
        import numpy as np
from pathlib import Path
from tqdm import tqdm
from scipy.io import loadmat
import random
import math
from numpy import genfromtxt
import resampy
import matplotlib.pyplot as plt

# this is not where the data is, but where we will save the process
data_root = Path("/data/stg60/transfer_learningdata/Milton/")
data_milton_par = Path("/data/stg60/transfer_learningdata/Milton/train/")
data_milton_pred = Path("/data/stg60/transfer_learningdata/Milton/test/")

def read_tsv(tsv, start_index = 1024, end_index = None, scale = True):
    dat = genfromtxt(tsv, delimiter='\t')
    x1,x2,x3,x4,x5,x6 = np.hsplit(dat,6)
    ell_1=((x1-x4)**2+(x2-x5)**2+(x3-x6)**2)**0.5 + 1e-10
    ang_sin_x_1=(x1-x4)/ell_1
    phi =np.arcsin(ang_sin_x_1)*180/np.pi
    phi = phi.reshape(-1)[start_index:end_index]
    x1 = x1.reshape(-1)[start_index:end_index]
    phi = resampy.resample(phi, sr_orig = 250, sr_new = 100)
    x1 = resampy.resample(x1, sr_orig = 250, sr_new = 100)
    if scale:
        phi = phi/20
        x1 = x1/0.335
    return phi, x1

def strides(a, L = 128, S=1):  # Window len = L, Stride len/stepsize = S
    tolist = False
    if isinstance(a, list):
        tolist = True
        a = np.array(a)
    if S is None:
        S = math.ceil(a.size/10)
    nrows = ((a.size - L) // S) + 1
    n = a.strides[0]
    windows = np.lib.stride_tricks.as_strided(a, shape=(nrows, L), strides=(S * n, n))
    
    if tolist:
        windows = windows.tolist()
    return windows

def getAllMats(path = "/data/stg60/new_miltondata/folder001/", sample = None, filter_required = True, L = 128, timetopredict = 1.96, samplingrate = 100):
    maxsize = int(timetopredict*samplingrate) + L*4 + 1
    data = sorted(Path(path).glob("milton*.mat"))
    if sample:
        data = random.sample(data, sample)
    if filter_required:

        data = list(filter(lambda x:abs(loadmat(x)['phiv']).max()>20 and loadmat(x)['phiv'].size>maxsize, data))
        #data = list(filter(lambda x:loadmat(x)['phiv'].max()>20, data))
    return data

def prepareData(path, cutoff = 128, delay = 23, L = 128, timetopredict = 1.96, samplingrate = 100, fall_stride = 1, start_index = 1024, end_index = None, scale = True):
    """AI is creating summary for prepareData

    Args:
        path ([Str]): [path to the original file]
        cutoff (int, optional): [description]. Defaults to 128.
        delay (int, optional): [description]. Defaults to 23.
        L (int, optional): [Window size]. Defaults to 128.
        timetopredict (float, optional): [description]. Defaults to 1.96.
        samplingrate (int, optional): [description]. Defaults to 100.
        fall_stride (int, optional): [description]. Defaults to 1.
        start_index (int, optional): [description]. Defaults to 1024.
        end_index ([type], optional): [description]. Defaults to None.
        scale (bool, optional): [description]. Defaults to True.
    Returns:
        [type]: [description]

    Yields:
        [type]: [description]
    """
    # timetopredict in seconds, note 1 point = 0.01 seconds, or 100points = 1seconds
    def disect(*args, separator = None):
        for arg in args:
            nofall, fall = arg[:-separator], arg[-separator:]
            yield (nofall, fall)
    
    if path.suffix == ".mat":    
        if not isinstance(path, str):
            path = path.as_posix()
        mat = loadmat(path)
        phiv = mat['phiv'].reshape(-1)
        dxv = mat['dxv'].reshape(-1)
        if scale:
            phiv = phiv/20
            dxv = dxv/0.335
    elif path.suffix == ".tsv":
        phiv, dxv = read_tsv(path, start_index = start_index, end_index=end_index)

    phiv = phiv[:-cutoff]
    dxv = dxv[:-cutoff]

    #phiv -> delay, respo
    delay_phiv = phiv[delay:]
    respo_phiv = phiv[:-delay]

    #dxv -> delay, respo
    delay_dxv = dxv[delay:]
    respo_dxv = dxv[:-delay]

    #fall region
    fall_region = L + int(timetopredict*samplingrate)
    [(n_delay_phiv, f_delay_phiv), (n_respo_phiv, f_respo_phiv)] = disect(delay_phiv, respo_phiv, separator=fall_region)
    [(n_delay_dxv, f_delay_dxv), (n_respo_dxv, f_respo_dxv)] = disect(delay_dxv, respo_dxv, separator=fall_region)

    if n_delay_phiv.size < f_delay_phiv.size:
        S = 1
    else:
        S = math.ceil((n_delay_phiv.size - f_delay_phiv.size)/(timetopredict*samplingrate+1))

    #s=1 for imbalanced
    # no_fall_windows
    n_delay_phiv = strides(n_delay_phiv, L = L, S = S)
    n_respo_phiv = strides(n_respo_phiv, L = L, S = S)
    n_delay_dxv = strides(n_delay_dxv, L = L, S = S)
    n_respo_dxv = strides(n_respo_dxv, L = L, S = S)

    # fall_windows
    f_delay_phiv = strides(f_delay_phiv, L = L, S = fall_stride)
    f_respo_phiv = strides(f_respo_phiv, L = L, S = fall_stride)
    f_delay_dxv = strides(f_delay_dxv, L = L, S = fall_stride)
    f_respo_dxv = strides(f_respo_dxv, L = L, S = fall_stride)

    # concatenate features
    n_x = np.stack((n_delay_phiv,n_respo_phiv,n_delay_dxv,n_respo_dxv), -1)
    f_x = np.stack((f_delay_phiv,f_respo_phiv,f_delay_dxv,f_respo_dxv), -1)

    n_y = np.zeros(n_x.shape[0]).reshape(-1,1)
    f_y = np.ones(f_x.shape[0]).reshape(-1,1)

    x = np.concatenate((n_x,f_x), 0)
    y = np.concatenate((n_y,f_y), 0)

    return x, y, S

def getOneHugeArray(path_list, start_index_list = None, end_index_list = None):
    datax = []
    datay = []
    if start_index_list is None and end_index_list is None:    
        for path in tqdm(path_list):
            x,y,_ = prepareData(path, cutoff = 128, delay = 23, L = 128, timetopredict = 1.96, samplingrate = 100, fall_stride = 1)
            datax.append(x)
            datay.append(y)
    else:
        for path, start_index, end_index in tqdm(zip(path_list, start_index_list, end_index_list)):
            x,y,_ = prepareData(path, cutoff = 128, delay = 23, L = 128, timetopredict = 1.96, samplingrate = 100, fall_stride = 1, start_index = start_index, end_index = end_index)
            datax.append(x)
            datay.append(y)
    x = np.concatenate(datax)    
    y = np.concatenate(datay)

    return x, y

def create_Real_Data(data_path_list, mode = "pred"):
    data_path_list = sorted(data_path_list.glob('*.tsv'))
    start_index_list = [1024]*len(data_path_list)
    end_index_list = [None]*len(data_path_list)
    for x in tqdm(data_path_list):
        phi,_ = read_tsv(x)
        plt.plot(phi, label = x.name)
    plt.legend()
    plt.savefig(f"plots_{mode}.png")

    x, y = getOneHugeArray(data_path_list, start_index_list = start_index_list, end_index_list = end_index_list)

    print(x.shape, y.shape, y.sum()/y.size)

    np.savez(data_root/f"{mode}.npz", x = x, y = y)


#def saveIndividually():
#    sim_path_list = getAllMats(sample=100)
#    train_path_list = sim_path_list[:-10]
#    val_path_list = sim_path_list[-10:]
#
#    data_root.mkdir(exist_ok=True)
#    train_target = data_root/"train"
#    train_target.mkdir(exist_ok = True)
#    val_target = data_root/"val"
#    val_target.mkdir(exist_ok=True)
#
#    savePerSimulation(train_path_list, train_target)
#    savePerSimulation(val_path_list, val_target)
#
#
#    meta = f"""
#    PER SIMULATION WISE, divided int o train and test folder
#    Data is sampled at 100,
#    Train test split is done by taking first 90 for train and 10 for test
#    we use dynamic stride size in fall region
#    no fall region uses only stride = 1
#    windows size = 128
#    data being used are from : simulated :  "/data/stg60/newchaos/data/mats/synthetic",
#    these data are filtered to remove size lesser than 1024 and that doesn't has fall that is angle should be greater than 20deg
#    """
#
#    with open(data_root/"metadata.txt", "w") as f:
#        f.write(meta)

if __name__ == '__main__':
    # """
    create_Real_Data(data_milton_pred, mode = "pred")
    create_Real_Data(data_milton_par, mode = "par")
    # """
    
    sim_path_list = getAllMats(sample=None)
    
    print("The sampled sim_path_list is ", len(sim_path_list))
    #print(sim_path_list)

    train_path_list = sim_path_list[:-int(len(sim_path_list)*0.05)]
    val_path_list = sim_path_list[-int(len(sim_path_list)*0.05):]

    xtrain, ytrain = getOneHugeArray(train_path_list)
    print(f"The shape of xtrain is {xtrain.shape}, the ratio of fall/nofall : {ytrain.sum()/ytrain.size}")
    xval, yval = getOneHugeArray(val_path_list)
    print(f"The shape of xval is {xval.shape}, the ratio of fall/nofall : {yval.sum()/yval.size}")

    data_root.mkdir(exist_ok=True)

    np.savez(data_root/"train.npz", x = xtrain, y = ytrain)
    np.savez(data_root/"val.npz", x = xval, y = yval)

    meta = f"""
    ONE-BIG-DATA for all
    Data is sampled at 100,
    Train test split is done by taking first 90 for train and 10 for test
    we use dynamic stride size in fall region
    no fall region uses only stride = 1
    windows size = 128
    data being used are from : simulated :  "/data/stg60/newchaos/data/mats/synthetic",
    these data are filtered to remove size lesser than 1024 and that doesn't has fall that is angle should be greater than 20deg
    """
    with open(data_root/"metadata.txt", "w") as f:
        f.write(meta)
    






        ########################################################
        ###############      train_newmilton.py      ###################
        ########################################################
        
        from silence_tensorflow import silence_tensorflow
silence_tensorflow()


import argparse
import os
import logging
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 
logging.getLogger('tensorflow').disabled = True

import numpy as np
from pathlib import Path
import tensorflow as tf
import sys
import os
from datetime import datetime

from sklearn.metrics import classification_report
from tensorflow.keras import regularizers 


#CUDA_VISIBLE_DEVICES=3,4,5,6,7 python train.py 2 24 222 train

#add these 2 lines because  [_Derived_]RecvAsync is cancelled. [[{{node replica_2/sequential/dense_4/BiasAdd/_377}}]] 
# [Op:__inference_predict_function_21214] error!!
#import os
#os.environ["TF_FORCE_GPU_ALLOW_GROWTH"]="true"

THRESHOLD = 0.5
tf.get_logger().setLevel('INFO')

assert os.environ.get('CUDA_VISIBLE_DEVICES'), """Please use CUDA_VISIBLE_DEVICES environment variable to run the experiment
Eg. CUDA_VISIBLE_DEVICES=1,2,3 python train.py --options"""

parser = argparse.ArgumentParser()
parser.add_argument('--version_data', required=True)
parser.add_argument('--version_model', required=True)
parser.add_argument('--version_catalog', required = True)
parser.add_argument('--num_gpu', required=True, type = int)
parser.add_argument('--sanity_check', default=1, type = int, choices=[0,1])
parser.add_argument('--epoch', default=100, type = int)
parser.add_argument('--sanity_epoch', default = 4, type = int)
parser.add_argument('--train_batch_size', default=128, type = int)
parser.add_argument('--test_batch_size', default=128, type = int)
parser.add_argument('--mode',required=True, choices = ['train', 'test'])
args = parser.parse_args()

version_catalog = args.version_catalog
version_data = args.version_data
version_model = args.version_model
train_batch_size = args.train_batch_size
test_batch_size = args.test_batch_size
sanity_check = args.sanity_check == 1

if sanity_check:
    print("The sanity check is enabled, running validation and training for following configs")
    limit = max(train_batch_size, test_batch_size) * 2
    print(f"Run for two batch, input size: {limit}")
    print(f"Sanity Epoch Check, epoch size:  {args.sanity_epoch}")
    print("!!!!!NOTE")
    print("Dsiable sanity_check flags to start training")
else:
    print(f"The sanity check is disabled, {args.mode} starting soon")
    limit = None

num_gpu = args.num_gpu
epoch = args.epoch if not sanity_check else args.sanity_epoch
mode = args.mode

if not sanity_check:    
    cat_root = "catalog"
    rep_root = "reports"
else:
    cat_root = rep_root = "temp"

if not sanity_check:
    assert not Path(f"{cat_root}/version_{version_catalog}").exists(), f"version-{version_catalog} Exists in the catalog"

gpulist = [f"/gpu:{i}" for i in range(num_gpu)]

strategy = tf.distribute.MirroredStrategy(devices=gpulist)
if not sanity_check:    
    checkpoint_root = f"/data/stg60/new_SavedModelKarl/version_{version_model}"
else:
    checkpoint_root = f"temp"

Path(checkpoint_root).mkdir(exist_ok=True)

checkpoint_path = f"{checkpoint_root}/model.ckpt"

root = Path(f"/data/stg60/karl_processeddata/version_{version_data}")

assert root.exists(), f"{root} doesn't exists"

with open(f"{cat_root}/version_{version_catalog}.txt", "w") as f:
    header = f"""
    DATE : {datetime.now().strftime("%d/%m/%Y %H:%M:%S")}
    version_catalog : {version_catalog}
    version_data : {version_data}
    version_model : {version_model}
    train_batch_size : {train_batch_size}
    test_batch_size : {test_batch_size}
    
    num_gpu : {num_gpu}
    limit : {limit}trainpath
    epoch : {epoch}
    mode : {mode}
    
    checkpoint_path : {checkpoint_path}
    
    data_root = {root}

    trainpath = {root} / 'train.npz'
    valpath = {root} / 'val.npz'
    testpath = {root} / 'par.npz'
    predpath = {root} / 'pred.npz'
    
    """
    
    with open("preparedata_newmilton.py", "r") as p:
        header_ = """
        ########################################################
        ###############       preparedata_newmilton.py       #############
        ########################################################
        
        """
        data_header = header_ + p.read()
        
    
    with open("train_newmilton.py", "r") as t:
        header_ = """
        ########################################################
        ###############      train_newmilton.py      ###################
        ########################################################
        
        """
        train_header = header_ + t.read()
    
    content = header + "\n" + data_header + "\n" + train_header
    
    f.write(content)

trainpath = root / 'train.npz'
valpath = root / 'val.npz'
testpath = root / 'karl_par.npz'
#predpath = root / 'pred.npz'

BATCH_SIZE = train_batch_size
TEST_BATCH_SIZE = test_batch_size
SHUFFLE_BUFFER_SIZE = BATCH_SIZE * 128

print('Train Loading')
with np.load(trainpath, allow_pickle=True) as data:
    train_examples = data['x'][:limit]
    train_labels = (data['y'][:limit]).astype(np.int64)

train_dataset = tf.data.Dataset.from_tensor_slices((train_examples, train_labels))
train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).prefetch(8)
print('Train Loaded')

print('Val Loading')
with np.load(valpath, allow_pickle=True) as data:
    val_examples = data['x']
    val_labels = (data['y']).astype(np.int64)

val_dataset = tf.data.Dataset.from_tensor_slices((val_examples, val_labels)).prefetch(8)
val_dataset = val_dataset.batch(BATCH_SIZE)
print('Val Loaded')


print('Test Loading')
with np.load(testpath, allow_pickle=True) as data:
    test_examples = data['x']
    test_labels = (data['y']).astype(np.int64)

test_dataset = tf.data.Dataset.from_tensor_slices((test_examples, test_labels)).prefetch(8)
test_dataset = test_dataset.batch(TEST_BATCH_SIZE)
print('Test Loaded')

# print('Pred Loading')
# with np.load(predpath, allow_pickle=True) as data:
#     pred_examples = data['x']
#     pred_labels = (data['y']).astype(np.int64)

# pred_dataset = tf.data.Dataset.from_tensor_slices((pred_examples, pred_labels)).prefetch(8)
# pred_dataset = test_dataset.batch(TEST_BATCH_SIZE)
# print('Pred Loaded')


options = tf.data.Options()
options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA
train_data = train_dataset.with_options(options)
val_dataset = val_dataset.with_options(options)
test_dataset = test_dataset.with_options(options)
# pred_dataset = pred_dataset.with_options(options)





print("Creating a model")
tf.keras.backend.set_floatx('float64')


def write_report(model, x, y,  target_path, on = "Par"):
    y_prob = tf.sigmoid(model.predict(x, batch_size=TEST_BATCH_SIZE))
    y_out = (y_prob > 0.5).numpy().astype("int32")
    target_names = ['No-Fall', 'Fall']
    print(f"Results on {on} Set")
    report = classification_report(y, y_out, target_names=target_names, zero_division = 0)

    if Path(target_path).exists():
            curr_acc = float(report.split('\n')[5].split()[1])
            with open(target_path,"r") as f:
                cache_acc = float(f.readlines()[5].split()[1])
            
            if curr_acc>=cache_acc:
                with open(target_path,"w") as f:
                    f.write(report)
    else:        
        with open(target_path,"w") as f:
            f.write(report)
    print(report)





class TestResult(tf.keras.callbacks.Callback):    
    def on_epoch_end(self, epoch, logs=None):
        name = f"{rep_root}/version_{version_catalog}_report_test.txt"
        write_report(model = self.model, x = test_examples, y = test_labels, target_path = name, on = "test")

        # name = f"{rep_root}/version_{version_catalog}_report_par_milton.txt"
        # write_report(model = self.model, x = test_examples, y = test_labels, target_path = name, on = "par")

        name = f"{rep_root}/version_{version_catalog}_report_val.txt"
        write_report(model = self.model, x = val_examples, y = val_labels, target_path = name, on = "val")
        
        

with strategy.scope():
    model = tf.keras.Sequential([
        tf.keras.layers.InputLayer(input_shape=(128, 4)),
        tf.keras.layers.LayerNormalization(
            axis=1, epsilon=1e-10, center=True, scale=True,
            beta_initializer='zeros', gamma_initializer='ones',
        ),
        
        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True)),
        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=False)),
        
        tf.keras.layers.BatchNormalization(axis=-1,
                                           momentum=0.99,
                                           epsilon=0.001,
                                           ),
        tf.keras.layers.Dense(128, activation='relu'),
        # tf.keras.layers.Dropout(0.5),
        tf.keras.layers.BatchNormalization(axis=-1,
                                           momentum=0.99,
                                           epsilon=0.001,
                                           ),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.BatchNormalization(axis=-1,
                                           momentum=0.99,
                                           epsilon=0.001,
                                           ),
        tf.keras.layers.Dense(32, activation='relu'),
        # tf.keras.layers.Dropout(0.5),
        tf.keras.layers.BatchNormalization(axis=-1,
                                           momentum=0.99,
                                           epsilon=0.001,
                                           ),
        tf.keras.layers.Dense(16, activation='relu',kernel_regularizer='l1'),
        tf.keras.layers.BatchNormalization(axis=-1,
                                           momentum=0.99,
                                           epsilon=0.001,
                                           ),
        tf.keras.layers.Dense(1)
    ])
    # model.load_weights(checkpoint_path)
    # regularizer = tf.keras.regularizers.l1(0.01)
    # for layer in model.layers:
    #  for attr in ['kernel_regularizer']:
    #     if hasattr(layer, attr):
    #       setattr(layer, attr, regularizer)
    #model.add(kernel_regularizer=l1(0.01), bias_regularizer=l1(0.01))
    model.compile(optimizer=tf.keras.optimizers.SGD(),
                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
    metrics = ['accuracy'])
    
    if mode == "test":
        model.load_weights(checkpoint_path)

callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)


# Create a callback that saves the model's weights
cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                 save_weights_only=True,
                                                 verbose=1,
                                                 save_best_only=True)

if mode == "train":
    print("model Fitting started")
    print("#"*200)
    print(f"TRAINING STATS : GPU-NUM : {num_gpu}, limit: {limit}, gpulist = {gpulist}, Epoch : {epoch}")
    print("#"*200)
    history = model.fit(train_dataset, 
                        validation_data=val_dataset, 
                        epochs=epoch, 
                        callbacks=[callback, 
                                cp_callback, 
                                TestResult(), 
                                tf.keras.callbacks.TensorBoard(
                                        log_dir=f'logs', histogram_freq=0, write_graph=True,
                                        write_images=False, update_freq='batch', profile_batch=2,
                                        embeddings_freq=0, embeddings_metadata=None
                                    )
                                    ]
                    )

    print("Model Fitting Done")
    print("Model testing Started")
    print(f"Test Performance Loss, Accuracy = {model.evaluate(test_dataset)}")
    print("Model Testing Done")
    """
    try:    
        print("Model Evaluation Started Val Set")
        y_prob = tf.sigmoid(model.predict(val_examples))
        y_out = (y_prob > 0.5).numpy().astype("int32")
    except Exception as e:
        print(e)
        sys.exit(e)
        
    target_names = ['No-Fall', 'Fall']
    report = classification_report(val_labels, y_out, target_names=target_names, zero_division = 0)
    with open(f"{rep_root}/version_{version_catalog}_report_val_milton.txt","w") as f:
        f.write(report)
    print(report)
    """
    
    
else:
    print("Model Evaluation Started")
    
    y_prob = tf.sigmoid(model.predict(test_examples))
    y_out = (y_prob > 0.5).numpy().astype("int32")

    target_names = ['No-Fall', 'Fall']
    report = classification_report(test_labels, y_out, target_names=target_names, zero_division = 0)
    
    with open(f"{rep_root}/test_report_version_{version_catalog}.txt","w") as f:
        f.write(report)
       
        
    
    print(report)


with open(f"{cat_root}/version_{version_catalog}.txt", "a") as f:
    results = """
    ########################################################################### 
    #########################        RESULTS       ############################ 
    ########################################################################### 
    """
    
    # par_path = f"{rep_root}/version_{version_catalog}_report_par_milton.txt"
    test_path = f"{rep_root}/version_{version_catalog}_report_test_karl.txt"
    val_path = f"{rep_root}/version_{version_catalog}_report_val_milton.txt"
    
    # with open(par_path, "r") as p:
    #     par_header = p.read()
    
    with open(test_path, "r") as p:
        test_header = p.read()
    
    with open(val_path, "r") as p:
        val_header = p.read()
    
    content = "\n" + results + "\n" + test_header + "\n" + val_header
    
    f.write(content)