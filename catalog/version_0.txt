
    DATE : 25/04/2022 02:05:09
    version_catalog : 0
    version_data : 0
    version_model : 0
    train_batch_size : 256
    test_batch_size : 256
    
    num_gpu : 8
    limit : None
    epoch : 4
    mode : train
    
    checkpoint_path : /data/keshav/CACHE/version_0/model.ckpt
    
    data_root = /data/keshav/DATA/version_0

    trainpath = /data/keshav/DATA/version_0 / 'train.npz'
    valpath = /data/keshav/DATA/version_0 / 'val.npz'
    testpath = /data/keshav/DATA/version_0 / 'par.npz'
    predpath = /data/keshav/DATA/version_0 / 'pred.npz'
    
    

        ########################################################
        ###############       preparedata.py       #############
        ########################################################
        
        import numpy as np
from pathlib import Path
from tqdm import tqdm
from scipy.io import loadmat
import random
import math
from numpy import genfromtxt
import resampy
import matplotlib.pyplot as plt

# this is not where the data is, but where we will save the process
data_root = Path("/data/keshav/DATA/version_0")
data_milton_par = Path("/data/keshav/newchaos/data/mats/real/Prediction_project/Milton_for_parameter_est")
data_milton_pred = Path("/data/keshav/newchaos/data/mats/real/Prediction_project/Milton_for_prediction")

def read_tsv(tsv, start_index = 1024, end_index = None, scale = True):
    dat = genfromtxt(tsv, delimiter='\t')
    x1,x2,x3,x4,x5,x6 = np.hsplit(dat,6)
    ell_1=((x1-x4)**2+(x2-x5)**2+(x3-x6)**2)**0.5 + 1e-10
    ang_sin_x_1=(x1-x4)/ell_1
    phi =np.arcsin(ang_sin_x_1)*180/np.pi
    phi = phi.reshape(-1)[start_index:end_index]
    x1 = x1.reshape(-1)[start_index:end_index]
    phi = resampy.resample(phi, sr_orig = 250, sr_new = 100)
    x1 = resampy.resample(x1, sr_orig = 250, sr_new = 100)
    if scale:
        phi = phi/20
        x1 = x1/0.335
    return phi, x1

def strides(a, L = 128, S=1):  # Window len = L, Stride len/stepsize = S
    tolist = False
    if isinstance(a, list):
        tolist = True
        a = np.array(a)
    if S is None:
        S = math.ceil(a.size/10)
    nrows = ((a.size - L) // S) + 1
    n = a.strides[0]
    windows = np.lib.stride_tricks.as_strided(a, shape=(nrows, L), strides=(S * n, n))
    
    if tolist:
        windows = windows.tolist()
    return windows

def getAllMats(path = "/data/stg60/newchaos/data/mats/synthetic", sample = 100, filter_required = True, L = 128, timetopredict = 1.96, samplingrate = 100):
    maxsize = int(timetopredict*samplingrate) + L*4 + 1
    data = sorted(Path(path).glob('*/*.mat'))
    if sample:
        data = random.sample(data, sample)
    if filter_required:
        data = list(filter(lambda x:loadmat(x)['phiv'].max()>20 and loadmat(x)['phiv'].size>maxsize, data))
    return data

def prepareData(path, cutoff = 128, delay = 23, L = 128, timetopredict = 1.96, samplingrate = 100, fall_stride = 1, start_index = 1024, end_index = None, scale = True):
    # timetopredict in seconds, note 1 point = 0.01 seconds, or 100points = 1seconds
    def disect(*args, separator = None):
        for arg in args:
            nofall, fall = arg[:-separator], arg[-separator:]
            yield (nofall, fall)
    
    if path.suffix == ".mat":    
        if not isinstance(path, str):
            path = path.as_posix()
        mat = loadmat(path)
        phiv = mat['phiv'].reshape(-1)
        dxv = mat['dxv'].reshape(-1)
        if scale:
            phiv = phiv/20
            dxv = dxv/0.335
    elif path.suffix == ".tsv":
        phiv, dxv = read_tsv(path, start_index = start_index, end_index=end_index)

    phiv = phiv[:-cutoff]
    dxv = dxv[:-cutoff]

    #phiv -> delay, respo
    delay_phiv = phiv[delay:]
    respo_phiv = phiv[:-delay]

    #dxv -> delay, respo
    delay_dxv = dxv[delay:]
    respo_dxv = dxv[:-delay]

    #fall region
    fall_region = L + int(timetopredict*samplingrate)
    [(n_delay_phiv, f_delay_phiv), (n_respo_phiv, f_respo_phiv)] = disect(delay_phiv, respo_phiv, separator=fall_region)
    [(n_delay_dxv, f_delay_dxv), (n_respo_dxv, f_respo_dxv)] = disect(delay_dxv, respo_dxv, separator=fall_region)

    if n_delay_phiv.size < f_delay_phiv.size:
        S = 1
    else:
        S = math.ceil((n_delay_phiv.size - f_delay_phiv.size)/(timetopredict*samplingrate+1))

    
    # no_fall_windows
    n_delay_phiv = strides(n_delay_phiv, L = L, S = S)
    n_respo_phiv = strides(n_respo_phiv, L = L, S = S)
    n_delay_dxv = strides(n_delay_dxv, L = L, S = S)
    n_respo_dxv = strides(n_respo_dxv, L = L, S = S)

    # fall_windows
    f_delay_phiv = strides(f_delay_phiv, L = L, S = fall_stride)
    f_respo_phiv = strides(f_respo_phiv, L = L, S = fall_stride)
    f_delay_dxv = strides(f_delay_dxv, L = L, S = fall_stride)
    f_respo_dxv = strides(f_respo_dxv, L = L, S = fall_stride)

    # concatenate features
    n_x = np.stack((n_delay_phiv,n_respo_phiv,n_delay_dxv,n_respo_dxv), -1)
    f_x = np.stack((f_delay_phiv,f_respo_phiv,f_delay_dxv,f_respo_dxv), -1)

    n_y = np.zeros(n_x.shape[0]).reshape(-1,1)
    f_y = np.ones(f_x.shape[0]).reshape(-1,1)

    x = np.concatenate((n_x,f_x), 0)
    y = np.concatenate((n_y,f_y), 0)

    return x, y, S

def getOneHugeArray(path_list, start_index_list = None, end_index_list = None):
    datax = []
    datay = []
    if start_index_list is None and end_index_list is None:    
        for path in tqdm(path_list):
            x,y,_ = prepareData(path, cutoff = 128, delay = 23, L = 128, timetopredict = 1.96, samplingrate = 100, fall_stride = 1)
            datax.append(x)
            datay.append(y)
    else:
        for path, start_index, end_index in tqdm(zip(path_list, start_index_list, end_index_list)):
            x,y,_ = prepareData(path, cutoff = 128, delay = 23, L = 128, timetopredict = 1.96, samplingrate = 100, fall_stride = 1, start_index = start_index, end_index = end_index)
            datax.append(x)
            datay.append(y)
    x = np.concatenate(datax)    
    y = np.concatenate(datay)

    return x, y

def create_Real_Data(data_path_list, mode = "pred"):
    data_path_list = sorted(data_path_list.glob('*.tsv'))
    start_index_list = [1024]*len(data_path_list)
    end_index_list = [None]*len(data_path_list)
    for x in tqdm(data_path_list):
        phi,_ = read_tsv(x)
        plt.plot(phi, label = x.name)
    plt.legend()
    plt.savefig(f"plots_{mode}.png")

    x, y = getOneHugeArray(data_path_list, start_index_list = start_index_list, end_index_list = end_index_list)

    print(x.shape, y.shape, y.sum()/y.size)

    np.savez(data_root/f"{mode}.npz", x = x, y = y)


#def saveIndividually():
#    sim_path_list = getAllMats(sample=100)
#    train_path_list = sim_path_list[:-10]
#    val_path_list = sim_path_list[-10:]
#
#    data_root.mkdir(exist_ok=True)
#    train_target = data_root/"train"
#    train_target.mkdir(exist_ok = True)
#    val_target = data_root/"val"
#    val_target.mkdir(exist_ok=True)
#
#    savePerSimulation(train_path_list, train_target)
#    savePerSimulation(val_path_list, val_target)
#
#
#    meta = f"""
#    PER SIMULATION WISE, divided int o train and test folder
#    Data is sampled at 100,
#    Train test split is done by taking first 90 for train and 10 for test
#    we use dynamic stride size in fall region
#    no fall region uses only stride = 1
#    windows size = 128
#    data being used are from : simulated :  "/data/stg60/newchaos/data/mats/synthetic",
#    these data are filtered to remove size lesser than 1024 and that doesn't has fall that is angle should be greater than 20deg
#    """
#
#    with open(data_root/"metadata.txt", "w") as f:
#        f.write(meta)

if __name__ == '__main__':
    # """
    create_Real_Data(data_milton_pred, mode = "pred")
    create_Real_Data(data_milton_par, mode = "par")
    # """
    
    sim_path_list = getAllMats(sample=2500)
    
    print("The sampled sim_path_list is ", len(sim_path_list))

    train_path_list = sim_path_list[:-int(len(sim_path_list)*0.05)]
    val_path_list = sim_path_list[-int(len(sim_path_list)*0.05):]

    xtrain, ytrain = getOneHugeArray(train_path_list)
    print(f"The shape of xtrain is {xtrain.shape}, the ratio of fall/nofall : {ytrain.sum()/ytrain.size}")
    xval, yval = getOneHugeArray(val_path_list)
    print(f"The shape of xval is {xval.shape}, the ratio of fall/nofall : {yval.sum()/yval.size}")

    data_root.mkdir(exist_ok=True)

    np.savez(data_root/"train.npz", x = xtrain, y = ytrain)
    np.savez(data_root/"val.npz", x = xval, y = yval)

    meta = f"""
    ONE-BIG-DATA for all
    Data is sampled at 100,
    Train test split is done by taking first 90 for train and 10 for test
    we use dynamic stride size in fall region
    no fall region uses only stride = 1
    windows size = 128
    data being used are from : simulated :  "/data/stg60/newchaos/data/mats/synthetic",
    these data are filtered to remove size lesser than 1024 and that doesn't has fall that is angle should be greater than 20deg
    """
    with open(data_root/"metadata.txt", "w") as f:
        f.write(meta)
    






        ########################################################
        ###############       train.py       ###################
        ########################################################
        
        from silence_tensorflow import silence_tensorflow
silence_tensorflow()

import argparse
import os
import logging
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 
logging.getLogger('tensorflow').disabled = True

import numpy as np
from pathlib import Path
import tensorflow as tf
import sys
import os
from datetime import datetime

from sklearn.metrics import classification_report

#CUDA_VISIBLE_DEVICES=3,4,5,6,7 python train.py 2 24 222 train

THRESHOLD = 0.5
tf.get_logger().setLevel('INFO')

assert os.environ.get('CUDA_VISIBLE_DEVICES'), """Please use CUDA_VISIBLE_DEVICES environment variable to run the experiment
Eg. CUDA_VISIBLE_DEVICES=1,2,3 python train.py --options"""

parser = argparse.ArgumentParser()
parser.add_argument('--version_data', required=True)
parser.add_argument('--version_model', required=True)
parser.add_argument('--version_catalog', required = True)
parser.add_argument('--num_gpu', required=True, type = int)
parser.add_argument('--sanity_check', default=1, type = int, choices=[0,1])
parser.add_argument('--epoch', default=100, type = int)
parser.add_argument('--sanity_epoch', default = 4, type = int)
parser.add_argument('--train_batch_size', default=128, type = int)
parser.add_argument('--test_batch_size', default=128, type = int)
parser.add_argument('--mode',required=True, choices = ['train', 'test'])
args = parser.parse_args()

version_catalog = args.version_catalog
version_data = args.version_data
version_model = args.version_model
train_batch_size = args.train_batch_size
test_batch_size = args.test_batch_size
sanity_check = args.sanity_check == 1

if sanity_check:
    print("The sanity check is enabled, running validation and training for following configs")
    limit = max(train_batch_size, test_batch_size) * 2
    print(f"Run for two batch, input size: {limit}")
    print(f"Sanity Epoch Check, epoch size:  {args.sanity_epoch}")
    print("!!!!!NOTE")
    print("Dsiable sanity_check flags to start training")
else:
    print(f"The sanity check is disabled, {args.mode} starting soon")
    limit = None

num_gpu = args.num_gpu
epoch = args.epoch if sanity_check else args.sanity_epoch
mode = args.mode

if not sanity_check:    
    cat_root = "catalog"
    rep_root = "reports"
else:
    cat_root = rep_root = "temp"

if not sanity_check:
    assert not Path(f"{cat_root}/version_{version_catalog}").exists(), f"version-{version_catalog} Exists in the catalog"

gpulist = [f"/gpu:{i}" for i in range(num_gpu)]

strategy = tf.distribute.MirroredStrategy(devices=gpulist)
if not sanity_check:    
    checkpoint_root = f"/data/keshav/CACHE/version_{version_model}"
else:
    checkpoint_root = f"temp"

Path(checkpoint_root).mkdir(exist_ok=True)

checkpoint_path = f"{checkpoint_root}/model.ckpt"

root = Path(f"/data/keshav/DATA/version_{version_data}")

assert root.exists(), f"{root} doesn't exists"

with open(f"{cat_root}/version_{version_catalog}.txt", "w") as f:
    header = f"""
    DATE : {datetime.now().strftime("%d/%m/%Y %H:%M:%S")}
    version_catalog : {version_catalog}
    version_data : {version_data}
    version_model : {version_model}
    train_batch_size : {train_batch_size}
    test_batch_size : {test_batch_size}
    
    num_gpu : {num_gpu}
    limit : {limit}
    epoch : {epoch}
    mode : {mode}
    
    checkpoint_path : {checkpoint_path}
    
    data_root = {root}

    trainpath = {root} / 'train.npz'
    valpath = {root} / 'val.npz'
    testpath = {root} / 'par.npz'
    predpath = {root} / 'pred.npz'
    
    """
    
    with open("preparedata.py", "r") as p:
        header_ = """
        ########################################################
        ###############       preparedata.py       #############
        ########################################################
        
        """
        data_header = header_ + p.read()
        
    
    with open("train.py", "r") as t:
        header_ = """
        ########################################################
        ###############       train.py       ###################
        ########################################################
        
        """
        train_header = header_ + t.read()
    
    content = header + "\n" + data_header + "\n" + train_header
    
    f.write(content)

trainpath = root / 'train.npz'
valpath = root / 'val.npz'
testpath = root / 'par.npz'
predpath = root / 'pred.npz'

BATCH_SIZE = train_batch_size
TEST_BATCH_SIZE = test_batch_size
SHUFFLE_BUFFER_SIZE = BATCH_SIZE * 128

print('Train Loading')
with np.load(trainpath, allow_pickle=True) as data:
    train_examples = data['x'][:limit]
    train_labels = (data['y'][:limit]).astype(np.int64)

train_dataset = tf.data.Dataset.from_tensor_slices((train_examples, train_labels))
train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).prefetch(8)
print('Train Loaded')

print('Val Loading')
with np.load(valpath, allow_pickle=True) as data:
    val_examples = data['x']
    val_labels = (data['y']).astype(np.int64)

val_dataset = tf.data.Dataset.from_tensor_slices((val_examples, val_labels)).prefetch(8)
val_dataset = val_dataset.batch(BATCH_SIZE)
print('Val Loaded')


print('Test Loading')
with np.load(testpath, allow_pickle=True) as data:
    test_examples = data['x']
    test_labels = (data['y']).astype(np.int64)

test_dataset = tf.data.Dataset.from_tensor_slices((test_examples, test_labels)).prefetch(8)
test_dataset = test_dataset.batch(TEST_BATCH_SIZE)
print('Test Loaded')

print('Pred Loading')
with np.load(predpath, allow_pickle=True) as data:
    pred_examples = data['x']
    pred_labels = (data['y']).astype(np.int64)

pred_dataset = tf.data.Dataset.from_tensor_slices((pred_examples, pred_labels)).prefetch(8)
pred_dataset = test_dataset.batch(TEST_BATCH_SIZE)
print('Pred Loaded')


options = tf.data.Options()
options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA
train_data = train_dataset.with_options(options)
val_dataset = val_dataset.with_options(options)
test_dataset = test_dataset.with_options(options)
pred_dataset = pred_dataset.with_options(options)





print("Creating a model")
tf.keras.backend.set_floatx('float64')



class TestResult(tf.keras.callbacks.Callback):
    def on_epoch_begin(self, epoch, logs=None):
        y_prob = tf.sigmoid(self.model.predict(test_examples, batch_size=TEST_BATCH_SIZE))
        y_out = (y_prob > 0.5).numpy().astype("int32")
        target_names = ['No-Fall', 'Fall']
        print("Results on Par Set")
        report = classification_report(test_labels, y_out, target_names=target_names, zero_division = 0)
        
        target_path = f"{rep_root}/version_{version_catalog}_report_par_milton.txt"
        
        if Path(target_path).exists():
            curr_acc = float(report.split('\n')[5].split()[1])
            with open(target_path,"r") as f:
                cache_acc = float(f.readlines()[5].split()[1])
            
            if curr_acc>cache_acc:
                with open(target_path,"w") as f:
                    f.write(report)
        else:        
            with open(target_path,"w") as f:
                f.write(report)
        print(report)
        
    def on_epoch_end(self, epoch, logs=None):
        y_prob = tf.sigmoid(self.model.predict(pred_examples, batch_size=TEST_BATCH_SIZE))
        y_out = (y_prob > 0.5).numpy().astype("int32")
        target_names = ['No-Fall', 'Fall']
        print("Results on Pred Set")
        report = classification_report(pred_labels, y_out, target_names=target_names, zero_division = 0)
        
        target_path = f"{rep_root}/version_{version_catalog}_report_pred_milton.txt"
        
        if Path(target_path).exists():
            curr_acc = float(report.split('\n')[5].split()[1])
            with open(target_path,"r") as f:
                cache_acc = float(f.readlines()[5].split()[1])
            
            if curr_acc>cache_acc:
                with open(target_path,"w") as f:
                    f.write(report)
        else:        
            with open(target_path,"w") as f:
                f.write(report)
        print(report)
        
        

with strategy.scope():
    model = tf.keras.Sequential([
        tf.keras.layers.InputLayer(input_shape=(128, 4)),
        tf.keras.layers.LayerNormalization(
            axis=1, epsilon=1e-10, center=True, scale=True,
            beta_initializer='zeros', gamma_initializer='ones',
        ),
        
        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=True)),
        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256, return_sequences=False)),
        
        tf.keras.layers.BatchNormalization(axis=-1,
                                           momentum=0.99,
                                           epsilon=0.001,
                                           ),
        tf.keras.layers.Dense(128, activation='relu'),
        # tf.keras.layers.Dropout(0.5),
        tf.keras.layers.BatchNormalization(axis=-1,
                                           momentum=0.99,
                                           epsilon=0.001,
                                           ),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.BatchNormalization(axis=-1,
                                           momentum=0.99,
                                           epsilon=0.001,
                                           ),
        tf.keras.layers.Dense(32, activation='relu'),
        # tf.keras.layers.Dropout(0.5),
        tf.keras.layers.BatchNormalization(axis=-1,
                                           momentum=0.99,
                                           epsilon=0.001,
                                           ),
        tf.keras.layers.Dense(16, activation='relu'),
        tf.keras.layers.BatchNormalization(axis=-1,
                                           momentum=0.99,
                                           epsilon=0.001,
                                           ),
        tf.keras.layers.Dense(1)
    ])

    model.compile(optimizer=tf.keras.optimizers.SGD(),
                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
    metrics = ['accuracy'])
    
    if mode == "test":
        model.load_weights(checkpoint_path)

callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)


# Create a callback that saves the model's weights
cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,
                                                 save_weights_only=True,
                                                 verbose=1,
                                                 save_best_only=True)

if mode == "train":
    print("model Fitting started")
    print("#"*200)
    print(f"TRAINING STATS : GPU-NUM : {num_gpu}, limit: {limit}, gpulist = {gpulist}, Epoch : {epoch}")
    print("#"*200)
    history = model.fit(train_dataset, 
                        validation_data=val_dataset, 
                        epochs=epoch, 
                        callbacks=[callback, 
                                cp_callback, 
                                TestResult(), 
                                tf.keras.callbacks.TensorBoard(
                                        log_dir=f'logs', histogram_freq=0, write_graph=True,
                                        write_images=False, update_freq='batch', profile_batch=2,
                                        embeddings_freq=0, embeddings_metadata=None
                                    )
                                    ]
                    )

    print("Model Fitting Done")
    print("Model testing Started")
    print(f"Test Performance Loss, Accuracy = {model.evaluate(test_dataset)}")
    print("Model Testing Done")
    
    try:    
        print("Model Evaluation Started Val Set")
        y_prob = tf.sigmoid(model.predict(val_examples))
        y_out = (y_prob > 0.5).numpy().astype("int32")
    except Exception as e:
        sys.exit(e)
        
    target_names = ['No-Fall', 'Fall']
    report = classification_report(val_labels, y_out, target_names=target_names, zero_division = 0)
    with open(f"{rep_root}/version_{version_catalog}_report_val_milton.txt","w") as f:
        f.write(report)
    print(report)
    
    
else:
    print("Model Evaluation Started")
    
    y_prob = tf.sigmoid(model.predict(test_examples))
    y_out = (y_prob > 0.84).numpy().astype("int32")

    target_names = ['No-Fall', 'Fall']
    report = classification_report(test_labels, y_out, target_names=target_names, zero_division = 0)
    
    with open(f"{rep_root}/test_report_lt10_0_85.txt","w") as f:
        f.write(report)
    
    print(report)


with open(f"{cat_root}/version_{version_catalog}.txt", "a") as f:
    results = """
    ########################################################################### 
    #########################        RESULTS       ############################ 
    ########################################################################### 
    """
    
    par_path = f"{rep_root}/version_{version_catalog}_report_par_milton.txt"
    pred_path = f"{rep_root}/version_{version_catalog}_report_pred_milton.txt"
    val_path = f"{rep_root}/version_{version_catalog}_report_val_milton.txt"
    
    with open(par_path, "r") as p:
        par_header = p.read()
    
    with open(pred_path, "r") as p:
        pred_header = p.read()
    
    with open(val_path, "r") as p:
        val_header = p.read()
    
    content = "\n" + results + "\n" + par_header + "\n" + pred_header + "\n" + val_header
    
    f.write(content)

